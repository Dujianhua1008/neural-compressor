{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Get Started Notebook of Intel® Neural Compressor for Pytorch\n",
    "\n",
    "\n",
    "This notebook is designed to provide an easy-to-follow guide for getting started with the [Intel® Neural Compressor](https://github.com/intel/neural-compressor) (INC) library for [pytorch](https://github.com/pytorch/pytorch) framework.\n",
    "\n",
    "In the following sections, we are going to use a DistilBert model fine-tuned on MRPC as an example to show how to apply post-training quantization on [transformers](https://github.com/huggingface/transformers) models using the INC library.\n",
    "\n",
    "\n",
    "The main objectives of this notebook are:\n",
    "\n",
    "1. Prerequisite: Prepare necessary environment, model and dataset.\n",
    "2. Quantization with INC: Walk through the step-by-step process of applying post-training quantization.\n",
    "3. Benchmark with INC: Evaluate the performance of the FP32 and INT8 models.\n",
    "\n",
    "\n",
    "## 1. Prerequisite\n",
    "\n",
    "### 1.1 Environment\n",
    "\n",
    "If you have Jupyter Notebook, you may directly run this notebook. We will use pip to install or upgrade [neural-compressor](https://github.com/intel/neural-compressor), [pytorch](https://github.com/pytorch/pytorch) and other required packages.\n",
    "\n",
    "Otherwise, you can setup a new environment. First, we install [Anaconda](https://www.anaconda.com/distribution/). Then open an Anaconda prompt window and run the following commands:\n",
    "\n",
    "```shell\n",
    "conda create -n inc_notebook python==3.8\n",
    "conda activate inc_notebook\n",
    "pip install jupyter\n",
    "jupyter notebook\n",
    "```\n",
    "The last command will launch Jupyter Notebook and we can open this notebook in browser to continue.\n",
    "\n",
    "Then, let's install necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/home/dujianhua/neural-compressor\n",
      "/data/home/dujianhua/neural-compressor\n"
     ]
    }
   ],
   "source": [
    "%cd ~/neural-compressor\n",
    "!pwd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "81.07s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "88.06s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/home/dujianhua/anaconda3/envs/inc_notebook/bin/python: can't open file '/data/home/dujianhua/neural-compressor/examples/notebook/pytorch/setup.py': [Errno 2] No such file or directory\n",
      "/data/home/dujianhua/neural-compressor/examples/notebook\n"
     ]
    }
   ],
   "source": [
    "# install neural-compressor from source\n",
    "import sys\n",
    "# !git clone https://github.com/intel/neural-compressor.git\n",
    "# %cd ./neural-compressor\n",
    "!{sys.executable} -m pip install -r requirements.txt\n",
    "!{sys.executable} setup.py install\n",
    "%cd ..\n",
    "\n",
    "# or install stable basic version from pypi\n",
    "# !{sys.executable} -m pip install neural-compressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# install other packages used in this notebook.\n",
    "!cd ./neural-compressor\n",
    "\n",
    "!{sys.executable} -m pip install -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Load Dataset\n",
    "\n",
    "The General Language Understanding Evaluation (GLUE) benchmark is a group of nine classification tasks on sentences or pairs of sentences which are:\n",
    "\n",
    "- [CoLA](https://nyu-mll.github.io/CoLA/) (Corpus of Linguistic Acceptability) Determine if a sentence is grammatically correct or not.\n",
    "- [MNLI](https://arxiv.org/abs/1704.05426) (Multi-Genre Natural Language Inference) Determine if a sentence entails, contradicts or is unrelated to a given hypothesis. This dataset has two versions, one with the validation and test set coming from the same distribution, another called mismatched where the validation and test use out-of-domain data.\n",
    "- [MRPC](https://www.microsoft.com/en-us/download/details.aspx?id=52398) (Microsoft Research Paraphrase Corpus) Determine if two sentences are paraphrases from one another or not.\n",
    "- [QNLI](https://rajpurkar.github.io/SQuAD-explorer/) (Question-answering Natural Language Inference) Determine if the answer to a question is in the second sentence or not. This dataset is built from the SQuAD dataset.\n",
    "- [QQP](https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs) (Quora Question Pairs2) Determine if two questions are semantically equivalent or not.\n",
    "- [RTE](https://aclweb.org/aclwiki/Recognizing_Textual_Entailment) (Recognizing Textual Entailment) Determine if a sentence entails a given hypothesis or not.\n",
    "- [SST-2](https://nlp.stanford.edu/sentiment/index.html) (Stanford Sentiment Treebank) Determine if the sentence has a positive or negative sentiment.\n",
    "- [STS-B](http://ixa2.si.ehu.es/stswiki/index.php/STSbenchmark) (Semantic Textual Similarity Benchmark) Determine the similarity of two sentences with a score from 1 to 5.\n",
    "- [WNLI](https://cs.nyu.edu/faculty/davise/papers/WinogradSchemas/WS.html) (Winograd Natural Language Inference) Determine if a sentence with an anonymous pronoun and a sentence with this pronoun replaced are entailed or not. This dataset is built from the Winograd Schema Challenge dataset.\n",
    "\n",
    "Here, we use MRPC task. We download and load the required dataset from hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 16:31:22.257636: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "import numpy as np\n",
    "import transformers\n",
    "from datasets import load_dataset, load_metric\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    EvalPrediction,\n",
    "    Trainer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset glue (/data/home/dujianhua/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b504d9c6ad3492abd45d4559763482a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "task_name = 'mrpc'\n",
    "raw_datasets = load_dataset(\"glue\", task_name)\n",
    "label_list = raw_datasets[\"train\"].features[\"label\"].names\n",
    "num_labels = len(label_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Prepare Model\n",
    "Download the pretrained model [textattack/distilbert-base-uncased-MRPC](https://huggingface.co/textattack/distilbert-base-uncased-MRPC) to a pytorch model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'textattack/distilbert-base-uncased-MRPC'\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=num_labels,\n",
    "    finetuning_task=task_name,\n",
    "    use_auth_token=None,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    use_auth_token=None,\n",
    ")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    from_tf=False,\n",
    "    config=config,\n",
    "    use_auth_token=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Dataset Preprocessing\n",
    "We need to preprocess the raw dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /data/home/dujianhua/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-24ad1972b8225650.arrow\n",
      "Loading cached processed dataset at /data/home/dujianhua/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-27c2369cdbd7171d.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ff1acd7b71e429596880e9115359ea0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1725 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentence1_key, sentence2_key = (\"sentence1\", \"sentence2\")\n",
    "padding = \"max_length\"\n",
    "label_to_id = None\n",
    "max_seq_length = 128\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    args = (\n",
    "        (examples[sentence1_key], examples[sentence2_key])\n",
    "    )\n",
    "    result = tokenizer(*args, padding=padding, max_length=max_seq_length, truncation=True)\n",
    "    return result\n",
    "\n",
    "raw_datasets = raw_datasets.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Quantization with Intel® Neural Compressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Define metric, evaluate function, and dataloader\n",
    "\n",
    "In this part, we define a GLUE metirc and use it to generate an evaluate function for INC.\n",
    "\n",
    "Refer to doc [metric.md](https://github.com/intel/neural-compressor/blob/master/docs/source/metric.md#build-custom-metric-with-python-api) for how to build your own metric.\n",
    "Refer to doc [dataset.md](https://github.com/intel/neural-compressor/blob/master/docs/source/dataset.md#user-specific-dataset) and [dataloader.md](https://github.com/intel/neural-compressor/blob/master/docs/source/dataloader.md#build-custom-dataloader-with-python-apiapi) for how to build your own dataset and dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3700073/2254047038.py:2: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"glue\", task_name)\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "eval_dataset = raw_datasets[\"validation\"]\n",
    "metric = load_metric(\"glue\", task_name)\n",
    "data_collator = None\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
    "    preds = np.argmax(preds, axis=1)\n",
    "    result = metric.compute(predictions=preds, references=p.label_ids)\n",
    "    if len(result) > 1:\n",
    "        result[\"combined_score\"] = np.mean(list(result.values())).item()\n",
    "    return result\n",
    "\n",
    "# Initialize our Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataset=None,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "eval_dataloader = trainer.get_eval_dataloader()\n",
    "\n",
    "# for transformers 4.31.0: accelerate dataloader\n",
    "# please use the code below to avoid error \n",
    "if eval_dataloader.batch_size is None:\n",
    "    def _build_inc_dataloader(dataloader):\n",
    "        class INCDataLoader:\n",
    "            __iter__ = dataloader.__iter__\n",
    "            def __init__(self) -> None:\n",
    "                self.dataloader = dataloader\n",
    "                self.batch_size = dataloader.total_batch_size\n",
    "        return INCDataLoader()\n",
    "    eval_dataloader = _build_inc_dataloader(eval_dataloader)\n",
    "batch_size = eval_dataloader.batch_size\n",
    "\n",
    "def take_eval_steps(model, trainer, save_metrics=False):\n",
    "    trainer.model = model\n",
    "    metrics = trainer.evaluate()\n",
    "    bert_task_acc_keys = ['eval_f1', 'eval_accuracy', 'eval_matthews_correlation',\n",
    "                            'eval_pearson', 'eval_mcc', 'eval_spearmanr']\n",
    "    for key in bert_task_acc_keys:\n",
    "        if key in metrics.keys():\n",
    "            throughput = metrics.get(\"eval_samples_per_second\")\n",
    "            print('Batch size = %d' % batch_size)\n",
    "            print(\"Finally Eval {} Accuracy: {}\".format(key, metrics[key]))\n",
    "            print(\"Latency: %.3f ms\" % (1000 / throughput))\n",
    "            print(\"Throughput: {} samples/sec\".format(throughput))\n",
    "            return metrics[key]\n",
    "    assert False, \"No metric returned, Please check inference metric!\"\n",
    "\n",
    "def eval_func(model):\n",
    "    return take_eval_steps(model, trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Run Quantization\n",
    "\n",
    "So far, we can finally start to quantize the model. \n",
    "\n",
    "To start, we need to set the configuration for post-training quantization using `PostTrainingQuantConfig` class. Once the configuration is set, we can proceed to the next step by calling the `quantization.fit()` function. This function performs the quantization process on the model and will return the best quantized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 16:32:57 [INFO] Start auto tuning.\n",
      "2023-10-31 16:32:57 [INFO] Execute the tuning process due to detect the evaluation function.\n",
      "2023-10-31 16:32:57 [INFO] Adaptor has 5 recipes.\n",
      "2023-10-31 16:32:57 [INFO] 0 recipes specified by user.\n",
      "2023-10-31 16:32:57 [INFO] 3 recipes require future tuning.\n",
      "2023-10-31 16:32:57 [INFO] *** Initialize auto tuning\n",
      "2023-10-31 16:32:57 [INFO] {\n",
      "2023-10-31 16:32:57 [INFO]     'PostTrainingQuantConfig': {\n",
      "2023-10-31 16:32:57 [INFO]         'AccuracyCriterion': {\n",
      "2023-10-31 16:32:57 [INFO]             'criterion': 'relative',\n",
      "2023-10-31 16:32:57 [INFO]             'higher_is_better': True,\n",
      "2023-10-31 16:32:57 [INFO]             'tolerable_loss': 0.01,\n",
      "2023-10-31 16:32:57 [INFO]             'absolute': None,\n",
      "2023-10-31 16:32:57 [INFO]             'keys': <bound method AccuracyCriterion.keys of <neural_compressor.config.AccuracyCriterion object at 0x7f3f980df3d0>>,\n",
      "2023-10-31 16:32:57 [INFO]             'relative': 0.01\n",
      "2023-10-31 16:32:57 [INFO]         },\n",
      "2023-10-31 16:32:57 [INFO]         'approach': 'post_training_static_quant',\n",
      "2023-10-31 16:32:57 [INFO]         'backend': 'default',\n",
      "2023-10-31 16:32:57 [INFO]         'calibration_sampling_size': [\n",
      "2023-10-31 16:32:57 [INFO]             100\n",
      "2023-10-31 16:32:57 [INFO]         ],\n",
      "2023-10-31 16:32:57 [INFO]         'device': 'cpu',\n",
      "2023-10-31 16:32:57 [INFO]         'diagnosis': False,\n",
      "2023-10-31 16:32:57 [INFO]         'domain': 'auto',\n",
      "2023-10-31 16:32:57 [INFO]         'example_inputs': None,\n",
      "2023-10-31 16:32:57 [INFO]         'excluded_precisions': [\n",
      "2023-10-31 16:32:57 [INFO]         ],\n",
      "2023-10-31 16:32:57 [INFO]         'framework': 'pytorch_fx',\n",
      "2023-10-31 16:32:57 [INFO]         'inputs': [\n",
      "2023-10-31 16:32:57 [INFO]         ],\n",
      "2023-10-31 16:32:57 [INFO]         'model_name': '',\n",
      "2023-10-31 16:32:57 [INFO]         'ni_workload_name': 'quantization',\n",
      "2023-10-31 16:32:57 [INFO]         'op_name_dict': None,\n",
      "2023-10-31 16:32:57 [INFO]         'op_type_dict': None,\n",
      "2023-10-31 16:32:57 [INFO]         'outputs': [\n",
      "2023-10-31 16:32:57 [INFO]         ],\n",
      "2023-10-31 16:32:57 [INFO]         'quant_format': 'default',\n",
      "2023-10-31 16:32:57 [INFO]         'quant_level': 'auto',\n",
      "2023-10-31 16:32:57 [INFO]         'recipes': {\n",
      "2023-10-31 16:32:57 [INFO]             'smooth_quant': False,\n",
      "2023-10-31 16:32:57 [INFO]             'smooth_quant_args': {\n",
      "2023-10-31 16:32:57 [INFO]             },\n",
      "2023-10-31 16:32:57 [INFO]             'layer_wise_quant': False,\n",
      "2023-10-31 16:32:57 [INFO]             'layer_wise_quant_args': {\n",
      "2023-10-31 16:32:57 [INFO]             },\n",
      "2023-10-31 16:32:57 [INFO]             'fast_bias_correction': False,\n",
      "2023-10-31 16:32:57 [INFO]             'weight_correction': False,\n",
      "2023-10-31 16:32:57 [INFO]             'gemm_to_matmul': True,\n",
      "2023-10-31 16:32:57 [INFO]             'graph_optimization_level': None,\n",
      "2023-10-31 16:32:57 [INFO]             'first_conv_or_matmul_quantization': True,\n",
      "2023-10-31 16:32:57 [INFO]             'last_conv_or_matmul_quantization': True,\n",
      "2023-10-31 16:32:57 [INFO]             'pre_post_process_quantization': True,\n",
      "2023-10-31 16:32:57 [INFO]             'add_qdq_pair_to_weight': False,\n",
      "2023-10-31 16:32:57 [INFO]             'optypes_to_exclude_output_quant': [\n",
      "2023-10-31 16:32:57 [INFO]             ],\n",
      "2023-10-31 16:32:57 [INFO]             'dedicated_qdq_pair': False,\n",
      "2023-10-31 16:32:57 [INFO]             'rtn_args': {\n",
      "2023-10-31 16:32:57 [INFO]             },\n",
      "2023-10-31 16:32:57 [INFO]             'awq_args': {\n",
      "2023-10-31 16:32:57 [INFO]             },\n",
      "2023-10-31 16:32:57 [INFO]             'gptq_args': {\n",
      "2023-10-31 16:32:57 [INFO]             },\n",
      "2023-10-31 16:32:57 [INFO]             'teq_args': {\n",
      "2023-10-31 16:32:57 [INFO]             }\n",
      "2023-10-31 16:32:57 [INFO]         },\n",
      "2023-10-31 16:32:57 [INFO]         'reduce_range': None,\n",
      "2023-10-31 16:32:57 [INFO]         'TuningCriterion': {\n",
      "2023-10-31 16:32:57 [INFO]             'max_trials': 600,\n",
      "2023-10-31 16:32:57 [INFO]             'objective': [\n",
      "2023-10-31 16:32:57 [INFO]                 'performance'\n",
      "2023-10-31 16:32:57 [INFO]             ],\n",
      "2023-10-31 16:32:57 [INFO]             'strategy': 'basic',\n",
      "2023-10-31 16:32:57 [INFO]             'strategy_kwargs': None,\n",
      "2023-10-31 16:32:57 [INFO]             'timeout': 0\n",
      "2023-10-31 16:32:57 [INFO]         },\n",
      "2023-10-31 16:32:57 [INFO]         'use_bf16': True\n",
      "2023-10-31 16:32:57 [INFO]     }\n",
      "2023-10-31 16:32:57 [INFO] }\n",
      "2023-10-31 16:32:57 [WARNING] [Strategy] Please install `mpi4py` correctly if using distributed tuning; otherwise, ignore this warning.\n",
      "/data/home/dujianhua/anaconda3/envs/inc_notebook/lib/python3.10/site-packages/torch/overrides.py:110: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  torch.has_cuda,\n",
      "/data/home/dujianhua/anaconda3/envs/inc_notebook/lib/python3.10/site-packages/torch/overrides.py:111: UserWarning: 'has_cudnn' is deprecated, please use 'torch.backends.cudnn.is_available()'\n",
      "  torch.has_cudnn,\n",
      "/data/home/dujianhua/anaconda3/envs/inc_notebook/lib/python3.10/site-packages/torch/overrides.py:117: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  torch.has_mps,\n",
      "/data/home/dujianhua/anaconda3/envs/inc_notebook/lib/python3.10/site-packages/torch/overrides.py:118: UserWarning: 'has_mkldnn' is deprecated, please use 'torch.backends.mkldnn.is_available()'\n",
      "  torch.has_mkldnn,\n",
      "2023-10-31 16:32:58 [INFO]  Found 6 blocks\n",
      "2023-10-31 16:32:58 [INFO] Attention Blocks: 6\n",
      "2023-10-31 16:32:58 [INFO] FFN Blocks: 6\n",
      "2023-10-31 16:32:58 [INFO] Pass query framework capability elapsed time: 902.22 ms\n",
      "2023-10-31 16:32:58 [INFO] Get FP32 model baseline.\n",
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/data/home/dujianhua/anaconda3/envs/inc_notebook/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7/7 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 16:33:43 [INFO] Save tuning history to /data/home/dujianhua/neural-compressor/examples/notebook/pytorch/nc_workspace/2023-10-31_16-32-55/./history.snapshot.\n",
      "2023-10-31 16:33:43 [INFO] FP32 baseline is: [Accuracy: 0.9027, Duration (seconds): 44.6601]\n",
      "2023-10-31 16:33:43 [INFO] Quantize the model with default config.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size = 64\n",
      "Finally Eval eval_f1 Accuracy: 0.9026845637583893\n",
      "Latency: 109.445 ms\n",
      "Throughput: 9.137 samples/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 16:33:43 [INFO] Fx trace of the entire model failed, We will conduct auto quantization\n",
      "/data/home/dujianhua/anaconda3/envs/inc_notebook/lib/python3.10/site-packages/torch/ao/quantization/observer.py:1354: UserWarning: Please use `is_dynamic` instead of `compute_dtype`.                     `compute_dtype` will be deprecated in a future release                     of PyTorch.\n",
      "  warnings.warn(\n",
      "/data/home/dujianhua/anaconda3/envs/inc_notebook/lib/python3.10/site-packages/torch/ao/quantization/observer.py:214: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n",
      "2023-10-31 16:33:44 [WARNING] Please note that calibration sampling size 100 isn't divisible exactly by batch size 64. So the real sampling size is 128.\n",
      "2023-10-31 16:33:46 [ERROR] Unexpected exception NotImplementedError(\"Could not run 'quantized::embedding_bag_prepack' with arguments from the 'QuantizedCUDA' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'quantized::embedding_bag_prepack' is only available for these backends: [QuantizedCPU, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradMPS, AutogradXPU, AutogradHPU, AutogradLazy, AutogradMeta, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\\n\\nQuantizedCPU: registered at /opt/conda/conda-bld/pytorch_1695392067780/work/aten/src/ATen/native/quantized/cpu/qembeddingbag_prepack.cpp:549 [kernel]\\nBackendSelect: fallthrough registered at /opt/conda/conda-bld/pytorch_1695392067780/work/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\\nPython: registered at /opt/conda/conda-bld/pytorch_1695392067780/work/aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\\nFuncTorchDynamicLayerBackMode: registered at /opt/conda/conda-bld/pytorch_1695392067780/work/aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\\nFunctionalize: registered at /opt/conda/conda-bld/pytorch_1695392067780/work/aten/src/ATen/FunctionalizeFallbackKernel.cpp:290 [backend fallback]\\nNamed: registered at /opt/conda/conda-bld/pytorch_1695392067780/work/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\\nConjugate: registered at /opt/conda/conda-bld/pytorch_1695392067780/work/aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\\nNegative: registered at /opt/conda/conda-bld/pytorch_1695392067780/work/aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\\nZeroTensor: registered at /opt/conda/conda-bld/pytorch_1695392067780/work/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\\nADInplaceOrView: fallthrough registered at /opt/conda/conda-bld/pytorch_1695392067780/work/aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\\nAutogradOther: registered at /opt/conda/conda-bld/pytorch_1695392067780/work/aten/src/ATen/core/VariableFallbackKernel.cpp:53 [backend fallback]\\nAutogradCPU: registered at /opt/conda/conda-bld/pytorch_1695392067780/work/aten/src/ATen/core/VariableFallbackKernel.cpp:57 [backend fallback]\\nAutogradCUDA: registered at /opt/conda/conda-bld/pytorch_1695392067780/work/aten/src/ATen/core/VariableFallbackKernel.cpp:65 [backend fallback]\\nAutogradXLA: registered at /opt/conda/conda-bld/pytorch_1695392067780/work/aten/src/ATen/core/VariableFallbackKernel.cpp:69 [backend fallback]\\nAutogradMPS: registered at /opt/conda/conda-bld/pytorch_1695392067780/work/aten/src/ATen/core/VariableFallbackKernel.cpp:77 [backend fallback]\\nAutogradXPU: registered at /opt/conda/conda-bld/pytorch_1695392067780/work/aten/src/ATen/core/VariableFallbackKernel.cpp:61 [backend fallback]\\nAutogradHPU: registered at /opt/conda/conda-bld/pytorch_1695392067780/work/aten/src/ATen/core/VariableFallbackKernel.cpp:90 [backend fallback]\\nAutogradLazy: registered at /opt/conda/conda-bld/pytorch_1695392067780/work/aten/src/ATen/core/VariableFallbackKernel.cpp:73 [backend fallback]\\nAutogradMeta: registered at /opt/conda/conda-bld/pytorch_1695392067780/work/aten/src/ATen/core/VariableFallbackKernel.cpp:81 [backend fallback]\\nTracer: registered at /opt/conda/conda-bld/pytorch_1695392067780/work/torch/csrc/autograd/TraceTypeManual.cpp:296 [backend fallback]\\nAutocastCPU: fallthrough registered at /opt/conda/conda-bld/pytorch_1695392067780/work/aten/src/ATen/autocast_mode.cpp:382 [backend fallback]\\nAutocastCUDA: fallthrough registered at /opt/conda/conda-bld/pytorch_1695392067780/work/aten/src/ATen/autocast_mode.cpp:249 [backend fallback]\\nFuncTorchBatched: registered at /opt/conda/conda-bld/pytorch_1695392067780/work/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:710 [backend fallback]\\nFuncTorchVmapMode: fallthrough registered at /opt/conda/conda-bld/pytorch_1695392067780/work/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\\nBatched: registered at /opt/conda/conda-bld/pytorch_1695392067780/work/aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\\nVmapMode: fallthrough registered at /opt/conda/conda-bld/pytorch_1695392067780/work/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\\nFuncTorchGradWrapper: registered at /opt/conda/conda-bld/pytorch_1695392067780/work/aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\\nPythonTLSSnapshot: registered at /opt/conda/conda-bld/pytorch_1695392067780/work/aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\\nFuncTorchDynamicLayerFrontMode: registered at /opt/conda/conda-bld/pytorch_1695392067780/work/aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\\nPreDispatch: registered at /opt/conda/conda-bld/pytorch_1695392067780/work/aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\\nPythonDispatcher: registered at /opt/conda/conda-bld/pytorch_1695392067780/work/aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\\n\") happened during tuning.\n",
      "Traceback (most recent call last):\n",
      "  File \"/data/home/dujianhua/anaconda3/envs/inc_notebook/lib/python3.10/site-packages/neural_compressor/quantization.py\", line 223, in fit\n",
      "    strategy.traverse()\n",
      "  File \"/data/home/dujianhua/anaconda3/envs/inc_notebook/lib/python3.10/site-packages/neural_compressor/strategy/auto.py\", line 134, in traverse\n",
      "    super().traverse()\n",
      "  File \"/data/home/dujianhua/anaconda3/envs/inc_notebook/lib/python3.10/site-packages/neural_compressor/strategy/strategy.py\", line 503, in traverse\n",
      "    q_model = self.adaptor.quantize(copy.deepcopy(tune_cfg), self.model, self.calib_dataloader, self.q_func)\n",
      "  File \"/data/home/dujianhua/anaconda3/envs/inc_notebook/lib/python3.10/site-packages/neural_compressor/utils/utility.py\", line 301, in fi\n",
      "    res = func(*args, **kwargs)\n",
      "  File \"/data/home/dujianhua/anaconda3/envs/inc_notebook/lib/python3.10/site-packages/neural_compressor/adaptor/pytorch.py\", line 3601, in quantize\n",
      "    PyTorch_FXAdaptor.convert_sub_graph(\n",
      "  File \"/data/home/dujianhua/anaconda3/envs/inc_notebook/lib/python3.10/site-packages/neural_compressor/adaptor/pytorch.py\", line 4287, in convert_sub_graph\n",
      "    PyTorch_FXAdaptor.convert_sub_graph(sub_module_list, module, op_name)\n",
      "  File \"/data/home/dujianhua/anaconda3/envs/inc_notebook/lib/python3.10/site-packages/neural_compressor/adaptor/pytorch.py\", line 4287, in convert_sub_graph\n",
      "    PyTorch_FXAdaptor.convert_sub_graph(sub_module_list, module, op_name)\n",
      "  File \"/data/home/dujianhua/anaconda3/envs/inc_notebook/lib/python3.10/site-packages/neural_compressor/adaptor/pytorch.py\", line 4283, in convert_sub_graph\n",
      "    module_con = convert_fx(module)\n",
      "  File \"/data/home/dujianhua/anaconda3/envs/inc_notebook/lib/python3.10/site-packages/torch/ao/quantization/quantize_fx.py\", line 596, in convert_fx\n",
      "    return _convert_fx(\n",
      "  File \"/data/home/dujianhua/anaconda3/envs/inc_notebook/lib/python3.10/site-packages/torch/ao/quantization/quantize_fx.py\", line 524, in _convert_fx\n",
      "    quantized = convert(\n",
      "  File \"/data/home/dujianhua/anaconda3/envs/inc_notebook/lib/python3.10/site-packages/torch/ao/quantization/fx/convert.py\", line 1091, in convert\n",
      "    model = lower_to_fbgemm(model, node_name_to_qconfig, node_name_to_scope)\n",
      "  File \"/data/home/dujianhua/anaconda3/envs/inc_notebook/lib/python3.10/site-packages/torch/ao/quantization/fx/lower_to_fbgemm.py\", line 16, in lower_to_fbgemm\n",
      "    return _lower_to_native_backend(model, qconfig_map, node_name_to_scope)\n",
      "  File \"/data/home/dujianhua/anaconda3/envs/inc_notebook/lib/python3.10/site-packages/torch/ao/quantization/fx/_lower_to_native_backend.py\", line 1156, in _lower_to_native_backend\n",
      "    _lower_weight_only_weighted_ref_module(model)\n",
      "  File \"/data/home/dujianhua/anaconda3/envs/inc_notebook/lib/python3.10/site-packages/torch/ao/quantization/fx/_lower_to_native_backend.py\", line 758, in _lower_weight_only_weighted_ref_module\n",
      "    q_module = q_class.from_reference(ref_module)  # type: ignore[union-attr]\n",
      "  File \"/data/home/dujianhua/anaconda3/envs/inc_notebook/lib/python3.10/site-packages/torch/ao/nn/quantized/modules/embedding_ops.py\", line 182, in from_reference\n",
      "    qembedding = cls(\n",
      "  File \"/data/home/dujianhua/anaconda3/envs/inc_notebook/lib/python3.10/site-packages/torch/ao/nn/quantized/modules/embedding_ops.py\", line 113, in __init__\n",
      "    self._packed_params.set_weight(qweight)\n",
      "  File \"/data/home/dujianhua/anaconda3/envs/inc_notebook/lib/python3.10/site-packages/torch/ao/nn/quantized/modules/embedding_ops.py\", line 30, in set_weight\n",
      "    self._packed_weight = torch.ops.quantized.embedding_bag_prepack(weight)\n",
      "  File \"/data/home/dujianhua/anaconda3/envs/inc_notebook/lib/python3.10/site-packages/torch/_ops.py\", line 692, in __call__\n",
      "    return self._op(*args, **kwargs or {})\n",
      "NotImplementedError: Could not run 'quantized::embedding_bag_prepack' with arguments from the 'QuantizedCUDA' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'quantized::embedding_bag_prepack' is only available for these backends: [QuantizedCPU, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradMPS, AutogradXPU, AutogradHPU, AutogradLazy, AutogradMeta, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n",
      "\n",
      "QuantizedCPU: registered at /opt/conda/conda-bld/pytorch_1695392067780/work/aten/src/ATen/native/quantized/cpu/qembeddingbag_prepack.cpp:549 [kernel]\n",
      "BackendSelect: fallthrough registered at /opt/conda/conda-bld/pytorch_1695392067780/work/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\n",
      "Python: registered at /opt/conda/conda-bld/pytorch_1695392067780/work/aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\n",
      "FuncTorchDynamicLayerBackMode: registered at /opt/conda/conda-bld/pytorch_1695392067780/work/aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\n",
      "Functionalize: registered at /opt/conda/conda-bld/pytorch_1695392067780/work/aten/src/ATen/FunctionalizeFallbackKernel.cpp:290 [backend fallback]\n",
      "Named: registered at /opt/conda/conda-bld/pytorch_1695392067780/work/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\n",
      "Conjugate: registered at /opt/conda/conda-bld/pytorch_1695392067780/work/aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\n",
      "Negative: registered at /opt/conda/conda-bld/pytorch_1695392067780/work/aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\n",
      "ZeroTensor: registered at /opt/conda/conda-bld/pytorch_1695392067780/work/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\n",
      "ADInplaceOrView: fallthrough registered at /opt/conda/conda-bld/pytorch_1695392067780/work/aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\n",
      "AutogradOther: registered at /opt/conda/conda-bld/pytorch_1695392067780/work/aten/src/ATen/core/VariableFallbackKernel.cpp:53 [backend fallback]\n",
      "AutogradCPU: registered at /opt/conda/conda-bld/pytorch_1695392067780/work/aten/src/ATen/core/VariableFallbackKernel.cpp:57 [backend fallback]\n",
      "AutogradCUDA: registered at /opt/conda/conda-bld/pytorch_1695392067780/work/aten/src/ATen/core/VariableFallbackKernel.cpp:65 [backend fallback]\n",
      "AutogradXLA: registered at /opt/conda/conda-bld/pytorch_1695392067780/work/aten/src/ATen/core/VariableFallbackKernel.cpp:69 [backend fallback]\n",
      "AutogradMPS: registered at /opt/conda/conda-bld/pytorch_1695392067780/work/aten/src/ATen/core/VariableFallbackKernel.cpp:77 [backend fallback]\n",
      "AutogradXPU: registered at /opt/conda/conda-bld/pytorch_1695392067780/work/aten/src/ATen/core/VariableFallbackKernel.cpp:61 [backend fallback]\n",
      "AutogradHPU: registered at /opt/conda/conda-bld/pytorch_1695392067780/work/aten/src/ATen/core/VariableFallbackKernel.cpp:90 [backend fallback]\n",
      "AutogradLazy: registered at /opt/conda/conda-bld/pytorch_1695392067780/work/aten/src/ATen/core/VariableFallbackKernel.cpp:73 [backend fallback]\n",
      "AutogradMeta: registered at /opt/conda/conda-bld/pytorch_1695392067780/work/aten/src/ATen/core/VariableFallbackKernel.cpp:81 [backend fallback]\n",
      "Tracer: registered at /opt/conda/conda-bld/pytorch_1695392067780/work/torch/csrc/autograd/TraceTypeManual.cpp:296 [backend fallback]\n",
      "AutocastCPU: fallthrough registered at /opt/conda/conda-bld/pytorch_1695392067780/work/aten/src/ATen/autocast_mode.cpp:382 [backend fallback]\n",
      "AutocastCUDA: fallthrough registered at /opt/conda/conda-bld/pytorch_1695392067780/work/aten/src/ATen/autocast_mode.cpp:249 [backend fallback]\n",
      "FuncTorchBatched: registered at /opt/conda/conda-bld/pytorch_1695392067780/work/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:710 [backend fallback]\n",
      "FuncTorchVmapMode: fallthrough registered at /opt/conda/conda-bld/pytorch_1695392067780/work/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\n",
      "Batched: registered at /opt/conda/conda-bld/pytorch_1695392067780/work/aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\n",
      "VmapMode: fallthrough registered at /opt/conda/conda-bld/pytorch_1695392067780/work/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n",
      "FuncTorchGradWrapper: registered at /opt/conda/conda-bld/pytorch_1695392067780/work/aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\n",
      "PythonTLSSnapshot: registered at /opt/conda/conda-bld/pytorch_1695392067780/work/aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\n",
      "FuncTorchDynamicLayerFrontMode: registered at /opt/conda/conda-bld/pytorch_1695392067780/work/aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\n",
      "PreDispatch: registered at /opt/conda/conda-bld/pytorch_1695392067780/work/aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\n",
      "PythonDispatcher: registered at /opt/conda/conda-bld/pytorch_1695392067780/work/aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n",
      "\n",
      "2023-10-31 16:33:46 [ERROR] Specified timeout or max trials is reached! Not found any quantized model which meet accuracy goal. Exit.\n"
     ]
    }
   ],
   "source": [
    "from neural_compressor.quantization import fit\n",
    "from neural_compressor.config import PostTrainingQuantConfig, TuningCriterion\n",
    "tuning_criterion = TuningCriterion(max_trials=600)\n",
    "conf = PostTrainingQuantConfig(approach=\"static\", tuning_criterion=tuning_criterion)\n",
    "q_model = fit(model, conf=conf, calib_dataloader=eval_dataloader, eval_func=eval_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Benchmark with Intel® Neural Compressor\n",
    "\n",
    "INC provides a benchmark feature to measure the model performance with the objective settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/bin/bash: {sys.executable}: command not found\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/bin/bash: {sys.executable}: command not found\n"
     ]
    }
   ],
   "source": [
    "# fp32 benchmark\n",
    "!{sys.executable} benchmark.py --input_model ./pytorch_model.bin 2>&1|tee fp32_benchmark.log\n",
    "\n",
    "# int8 benchmark\n",
    "!{sys.executable} benchmark.py --input_model ./saved_results/best_model.pt 2>&1|tee int8_benchmark.log\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
